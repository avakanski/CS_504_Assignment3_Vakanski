{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "7FGJ0Ou3577l",
        "6wLxL0BUAnIv",
        "_PCmjBvZBNMS",
        "Tp2zIBuAA8nE",
        "EJ0yDkzZEZXG",
        "CGD8ovXdFXgX",
        "BIwQfxezKOam",
        "sORSEAsk6PJL"
      ],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPcbEH6MiZ8IpsKmdq7lDmS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jstarace/CS_504_Assignment3_Vakanski/blob/main/CS504_Vakanski_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "7FGJ0Ou3577l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The following is a complete walk-through of Assignment #3 for CS 504: ST Adversarial Machine Learning, taught by Dr. Vakanski at the University of Idaho in the Spring of 2023.  The assignment is being made available to assist in the understanding of future related assignments at the request of Dr. Vakanski."
      ],
      "metadata": {
        "id": "iKQu5ga14pH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The assignment is broken down into 2 parts.\n",
        "1. Adversarial Traning defense against evasion attacks\n",
        "1. Poisoning attacks against deep learning classification models\n",
        "\n",
        "For this assignment I used Google Colab for the training of the models and completion of the assignment.  It is recommend that the same or similar is used."
      ],
      "metadata": {
        "id": "VCXqDS025Q7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook Setup"
      ],
      "metadata": {
        "id": "XwxhxH6T6Y6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Configuration\n",
        "\n",
        "Before diving into the assignment we want to make sure our environment has all the necessary libraries and connections"
      ],
      "metadata": {
        "id": "BB_vMXyN6cXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google Drive.\n",
        "# All images required for the assignment were either provided by Dr. Vakanski or were publicly available\n",
        "# Images were loaded onto google drive for ease of access\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uyw2vHOP6738"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the adversarial robustness toolbox so that we can work with adversarial models\n",
        "!pip install adversarial-robustness-toolbox"
      ],
      "metadata": {
        "id": "GopOSkwF7mXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "Q7QVvtLT8AmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For inline matplot use\n",
        "%matplotlib inline\n",
        "\n",
        "# Import the following libraries\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import sys\n",
        "import keras\n",
        "import random\n",
        "import imageio\n",
        "import tarfile\n",
        "import natsort\n",
        "import warnings\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras.backend as k\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "dSAySjDI76sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From installed and available libraries import these specific classes\n",
        "\n",
        "from os import listdir\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping \n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.tools.docs.doc_controls import T\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.python.ops.math_ops import TruncateDiv\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, GlobalAveragePooling2D\n",
        "\n",
        "from art.estimators.classification import KerasClassifier\n",
        "from art.estimators.classification import SklearnClassifier\n",
        "\n",
        "from art.attacks.evasion import BoundaryAttack\n",
        "from art.attacks.evasion import FastGradientMethod,ProjectedGradientDescent,DeepFool\n",
        "from art.utils import to_categorical\n",
        "\n",
        "from skimage.util import compare_images\n",
        "from IPython.display import clear_output\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from skimage import data, transform, exposure\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "from re import U"
      ],
      "metadata": {
        "id": "QLo1pKO-8MGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This line is important, without it the attacks in the ART toolbox won't work\n",
        "# And it needs to be run before training the models\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# Supress warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "B6EctZ2H8gvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the version of tensorflow and keras\n",
        "print(\"TensorFlow version:{}\".format(tf.__version__))\n",
        "print(\"Keras version:{}\".format(keras.__version__))"
      ],
      "metadata": {
        "id": "lHjpgYCZ8y-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definitions"
      ],
      "metadata": {
        "id": "alMmmYmJ9Ybm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following are the definitions for methods we will use during the assignment"
      ],
      "metadata": {
        "id": "yr0pHzMOAfFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plotting loss and accuracy"
      ],
      "metadata": {
        "id": "6wLxL0BUAnIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple method used to print a models accuracy and loss.  Pass the method\n",
        "# the history and both accuracy and loss will be displayed\n",
        "\n",
        "def plot_accuracy_loss(the_history):\n",
        "    # plot the accuracy and loss\n",
        "    train_loss = the_history.history['loss']\n",
        "    val_loss = the_history.history['val_loss']\n",
        "    acc = the_history.history['accuracy'] \n",
        "    val_acc = the_history.history['val_accuracy']\n",
        "\n",
        "    epochsn = np.arange(1, len(train_loss)+1,1)\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(epochsn, acc, 'b', label='Training Accuracy')\n",
        "    plt.plot(epochsn, val_acc, 'r', label='Validation Accuracy')\n",
        "    plt.grid(color='gray', linestyle='--')\n",
        "    plt.legend()            \n",
        "    plt.title('ACCURACY')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(epochsn,train_loss, 'b', label='Training Loss')\n",
        "    plt.plot(epochsn,val_loss, 'r', label='Validation Loss')\n",
        "    plt.grid(color='gray', linestyle='--')\n",
        "    plt.legend()\n",
        "    plt.title('LOSS')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5sWCVokx9nBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Definitions"
      ],
      "metadata": {
        "id": "GknbIw2kAye7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# For this assignment transfer learning was used for the base models.\n",
        "# Specifically resnet models were used.  You may note that when using the\n",
        "# ResNet models provided by Keras, their documentation mentions preprocessing\n",
        "# the images.  This is intentionally left out as it caused issues with the\n",
        "# accuracy of the predicitions.\n",
        "\n",
        "# As the inputs and classes may change we pass those as variables to provide a\n",
        "# more robust method for use.\n",
        "def build_resnet50(input_shape, class_count):\n",
        "  \n",
        "  #Transfer the model into a generic base model\n",
        "  baseModel = tf.keras.applications.ResNet50V2(\n",
        "      include_top = False,\n",
        "      weights=\"imagenet\",\n",
        "      input_shape = input_shape,\n",
        "      pooling = \"avg\",\n",
        "      classes = class_count\n",
        "  )\n",
        "\n",
        "  # Set each layer of the base model to trainable so it can be customized to our\n",
        "  # datasets\n",
        "  for layer in baseModel.layers:\n",
        "    layer.trainable = True\n",
        "  \n",
        "  # Create a new sequential model\n",
        "  model = Sequential()\n",
        "\n",
        "  # Add the transfered model\n",
        "  model.add(baseModel)\n",
        "\n",
        "  # Customize the final layers of the ResNet model by adding additional layers\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1024,\n",
        "                  #bias_regularizer = regularizers.L2(1e-4),\n",
        "                  activation = 'relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "  #model.add(Dense(512,\n",
        "  #                #bias_regularizer = regularizers.L2(1e-4),\n",
        "  #                activation = 'relu'))\n",
        "  #model.add(BatchNormalization())\n",
        "  #model.add(Dropout(0.3))\n",
        "  model.add(Dense(128,\n",
        "                  #bias_regularizer = regularizers.L2(1e-4),\n",
        "                  activation = 'relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(64,\n",
        "                  #bias_regularizer = regularizers.L2(1e-4),\n",
        "                  activation = 'relu'))\n",
        "  #model.add(Dense(1024,\n",
        "  #                #bias_regularizer = regularizers.L2(1e-4),\n",
        "  #                activation = 'relu'))\n",
        "  #model.add(BatchNormalization())\n",
        "  #model.add(Dropout(0.5))\n",
        "  #model.add(Dense(512,\n",
        "  #                #bias_regularizer = regularizers.L2(1e-4),\n",
        "  #                activation = 'relu'))\n",
        "  #model.add(BatchNormalization())\n",
        "  #model.add(Dropout(0.4))\n",
        "  #model.add(Dense(256,\n",
        "  #                #bias_regularizer = regularizers.L2(1e-4),\n",
        "  #                activation = 'relu'))\n",
        "  #model.add(BatchNormalization())\n",
        "  #model.add(Dropout(0.3))\n",
        "  #model.add(Dense(128,\n",
        "  #                #bias_regularizer = regularizers.L2(1e-4),\n",
        "  #                activation = 'relu'))\n",
        "  #model.add(BatchNormalization())\n",
        "  #model.add(Dropout(0.3))\n",
        "  #model.add(Dense(64,\n",
        "  #                #bias_regularizer = regularizers.L2(1e-4),\n",
        "  #                activation = 'relu'))\n",
        "  model.add(Dense(class_count, activation = 'softmax', name = 'output'))\n",
        "\n",
        "  return model\n",
        "\n",
        "# See comments above\n",
        "def build_resnet152(input_shape, class_count):\n",
        "  baseModel = tf.keras.applications.ResNet152V2(\n",
        "      include_top = False,\n",
        "      weights=\"imagenet\",\n",
        "      input_shape = input_shape,\n",
        "      pooling = 'avg',\n",
        "      classes = class_count,\n",
        "  )\n",
        "\n",
        "  for layer in baseModel.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(baseModel)\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(2048,\n",
        "                  bias_regularizer = regularizers.L2(1e-4),\n",
        "                  activation = 'relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(1024,\n",
        "                  bias_regularizer = regularizers.L2(1e-4),\n",
        "                  activation = 'relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(512,\n",
        "                  bias_regularizer = regularizers.L2(1e-4),\n",
        "                  activation = 'relu'))\n",
        "  model.add(Dense(256,\n",
        "                  bias_regularizer = regularizers.L2(1e-4),\n",
        "                  activation = 'relu'))\n",
        "  model.add(Dense(class_count, activation = 'softmax', name = 'output'))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "UsV4q-M0A1Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attack Definitions"
      ],
      "metadata": {
        "id": "toAATqNYA4x_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pgdAttack(eps, classifier, imgTest, targeted, verbose, labels=None):\n",
        "  attackPgd = ProjectedGradientDescent(estimator = classifier,\n",
        "                                       norm = np.inf,\n",
        "                                       eps = eps,\n",
        "                                       eps_step = 0.01,\n",
        "                                       max_iter = 200,\n",
        "                                       targeted = targeted,\n",
        "                                       verbose = verbose,\n",
        "                                       batch_size = 256)\n",
        "  if targeted == True and labels is not None:\n",
        "    return attackPgd.generate(imgTest, labels)\n",
        "  elif targeted == False:\n",
        "    return attackPgd.generate(imgTest)\n",
        "\n",
        "def generate_pgd_img(classifier, provided_array):\n",
        "  attack_pgd = ProjectedGradientDescent(estimator = classifier, eps = 20.0/255.0, eps_step = 0.01, max_iter = 100, targeted = False, verbose = False)\n",
        "  pgd_array = []\n",
        "  for x in provided_array:\n",
        "    img = attack_pgd.generate(x.reshape(1, 224, 224, 3))\n",
        "    img = img[0]\n",
        "    pgd_array.append(img)\n",
        "  pgd_array = np.array(pgd_array, dtype = np.float32)\n",
        "  return pgd_array\n",
        "\n",
        "def generate_fgsm_img(classifier, provided_array):\n",
        "  attack_fgsm = FastGradientMethod(estimator = classifier, eps = 20.0/255.0)\n",
        "  fgsm_array = []\n",
        "  for x in provided_array:\n",
        "    img = attack_fgsm.generate(x.reshape(1, 224, 224, 3))\n",
        "    img = img[0]\n",
        "    fgsm_array.append(img)\n",
        "  fgsm_array = np.array(fgsm_array, dtype = np.float32)\n",
        "  return fgsm_array\n",
        "\n",
        "def generate_df_img(classifier, provided_array):\n",
        "  attack_df = DeepFool(classifier = classifier, max_iter = 100, verbose = False)\n",
        "  df_array = []\n",
        "  for x in provided_array:\n",
        "    img = attack_df.generate(x.reshape(1, 224, 224, 3))\n",
        "    img = img[0]\n",
        "    df_array.append(img)\n",
        "  df_array = np.array(df_array, dtype = np.float32)\n",
        "  return df_array"
      ],
      "metadata": {
        "id": "0TY9_H51BAsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper methods for randomization"
      ],
      "metadata": {
        "id": "_PCmjBvZBNMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate an array of unique random ints used for selecting random indices\n",
        "def random_array(c, upper):\n",
        "  random_list = []\n",
        "  while len(random_list) < c:\n",
        "    #for i in range(c):\n",
        "    r = random.randint(0, upper-1)\n",
        "    if r not in random_list:\n",
        "      random_list.append(r)\n",
        "  return random_list\n",
        "\n",
        "# Get a subset from provided array\n",
        "def get_subset(subset, provided_images, provided_labels):\n",
        "  temp_img = []\n",
        "  temp_labels = []\n",
        "  for x in subset:\n",
        "    temp_img.append(provided_images[x])\n",
        "    temp_labels.append(provided_labels[x])\n",
        "  temp_img = np.array(temp_img, dtype = np.float32)\n",
        "  temp_labels = np.array(temp_labels, dtype = np.float32)\n",
        "  return temp_img, temp_labels\n",
        "\n",
        "# Get predictions for any model\n",
        "def generate_predictions(model, images):\n",
        "  labels = []\n",
        "  i = 0\n",
        "  for x in images:\n",
        "    labels.append(np.argmax(model.predict(images[i].reshape(1, 224, 224, 3))))\n",
        "    i += 1\n",
        "  labels = np.array(labels, dtype = np.float32)\n",
        "  return labels "
      ],
      "metadata": {
        "id": "HFSZ3pw1BW9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Image Loader"
      ],
      "metadata": {
        "id": "Tp2zIBuAA8nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(directory):\n",
        "\n",
        "    imgs_list = []\n",
        "\n",
        "    # List of all images in the directory\n",
        "    imgs_list_1 = listdir(directory)\n",
        "    # Make sure that the images are sorted \n",
        "    imagesList = natsort.natsorted(imgs_list_1)\n",
        "\n",
        "    # Read the images as numpy arrays\n",
        "    for i in range(len(imagesList)):\n",
        "          tmp_img = cv2.imread(os.path.join(directory, imagesList[i]))\n",
        "          img_arr = np.array(tmp_img)\n",
        "          imgs_list.append(img_arr/255.)\n",
        "    \n",
        "    # Convert the lists to numpy arrays\n",
        "    imgs = np.asarray(imgs_list)\n",
        "\n",
        "    return imgs"
      ],
      "metadata": {
        "id": "IMk9XSm8EHDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global variables"
      ],
      "metadata": {
        "id": "EJ0yDkzZEZXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoder for loading in data\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Define a save path for use when training models\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/CS_504/Assignment_3/Saves/'\n",
        "\n",
        "# Define the path for weights to be stored\n",
        "weight_path = 'Weights/'\n",
        "\n",
        "# Define the path for the model to be stored\n",
        "model_path = 'Models/'\n",
        "\n",
        "# Define the extention for weights/models\n",
        "ext = '.h5'"
      ],
      "metadata": {
        "id": "OO4w5ODLEdrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1"
      ],
      "metadata": {
        "id": "zpFxsgGk5_x_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1"
      ],
      "metadata": {
        "id": "atQMw5CN6D68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About the data\n",
        "\n",
        "For this part of the assignment we will be using a subset of the [WikiArt Dataset](https://paperswithcode.com/dataset/wikiart) that contains 3,988 paintings by 10 artists.  The images are stored in a directory 'Paintings.zip' and the labels in a csv file 'labels_paintings.csv'.\n",
        "\n",
        "We will be using 20% of the data for the test set and another 20% of the data for a validation set."
      ],
      "metadata": {
        "id": "exOHUkbgGBUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "CGD8ovXdFXgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncompress the dataset \n",
        "!unzip -uq \"/content/drive/MyDrive/Colab Notebooks/CS_504/Assignment_3/data/Paintings.zip\" -d \"sample_data/\""
      ],
      "metadata": {
        "id": "0lpYhUWqFfOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the images\n",
        "all_images = load_images('sample_data/Paintings')\n",
        "print('Shape of the images:', all_images.shape)\n",
        "\n",
        "# Load the labels\n",
        "all_labels = np.loadtxt('/content/drive/MyDrive/Colab Notebooks/CS_504/Assignment_3/data/labels_paintings.csv', delimiter=',', dtype=str)\n",
        "image_labels = le.fit_transform(all_labels)\n",
        "print('Shape of the labels:', image_labels.shape)"
      ],
      "metadata": {
        "id": "F9hEhbMoFkXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into 2 sets of data\n",
        "# 1. A trainval set that will later be split into the training and validation sets\n",
        "# 2. A test set which will be 20% of all data\n",
        "\n",
        "# By setting the random state, we can ensure consistency when creating the datasets\n",
        "# This allows us to focus on fine tuning our models\n",
        "trainval_images, test_images, trainval_labels, test_labels = train_test_split(all_images, image_labels, test_size=0.2, random_state=12)\n",
        "\n",
        "# Using the trainval sets, we split the data again into our training and validation sets\n",
        "# The validation set will be 20% of the remaining data and the remainder will be \n",
        "# our training set\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(trainval_images, trainval_labels, test_size=0.2, random_state=12)\n",
        "\n",
        "# With all the necessary info loaded, we can free up space by deleting the imported images\n",
        "del all_images"
      ],
      "metadata": {
        "id": "-V1kY9B4G6UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the shapes of train, validation, and test datasets\n",
        "print('Images train shape: {} - Labels train shape: {}'.format(train_images.shape, train_labels.shape))\n",
        "print('Images validation shape: {} - Labels validation shape: {}'.format(val_images.shape, val_labels.shape))\n",
        "print('Images test shape: {} - Labels test shape: {}'.format(test_images.shape, test_labels.shape))\n",
        "\n",
        "# Display the range of images \n",
        "print('\\nMax pixel value', np.max(train_images))\n",
        "print('Min pixel value', np.min(train_images))\n",
        "print('Average pixel value', np.mean(train_images))\n",
        "print('Data type', train_images[0].dtype)\n",
        "\n",
        "# A list with the names of the image classes\n",
        "label_names = np.unique(all_labels)"
      ],
      "metadata": {
        "id": "6hATfhMPH5hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot images with ground-truth labels"
      ],
      "metadata": {
        "id": "BIwQfxezKOam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(9, 9))\n",
        "for n in range(9):\n",
        "    i = np.random.randint(0, len(train_images), 1)\n",
        "    ax = plt.subplot(3, 3, n+1)\n",
        "    plt.imshow(train_images[i[0]])\n",
        "    plt.title('Ground Truth: ' + str(label_names[train_labels[i[0]]]))\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "9uwpw6l_KUjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a deep learning model above 70% accuracy"
      ],
      "metadata": {
        "id": "EKhAcRdBLAjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paths and variables"
      ],
      "metadata": {
        "id": "Fvb6b1HXLLyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight_name = 'atd_weights_'\n",
        "atd = 'atd/'"
      ],
      "metadata": {
        "id": "PbjL4a85LIe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = (224, 224, 3)\n",
        "NUM_CLASSES = len(label_names)\n",
        "batch = 32\n",
        "epochs = 100\n",
        "iterations = 3"
      ],
      "metadata": {
        "id": "LqeY2tSrLIMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adam = tf.keras.optimizers.legacy.Adam(learning_rate = .0001)\n",
        "\n",
        "model_monitor = EarlyStopping(monitor='val_loss',\n",
        "                            min_delta=1e-3,\n",
        "                            patience=30,\n",
        "                            verbose=0,\n",
        "                            mode='auto')"
      ],
      "metadata": {
        "id": "OyV9gOQKLIWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the model"
      ],
      "metadata": {
        "id": "kSXrqhBkLkmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Because initialization of the weights is random, we will loop through model \n",
        "# training based on the iteration value set above.  All accuracies and histories\n",
        "# will be stored in the arrays we create.  Later we will compare values and load\n",
        "# the model with the best performance\n",
        "accuracy = []\n",
        "histories = []\n",
        "\n",
        "for i in range(iterations):\n",
        "  model = build_resnet50(input, NUM_CLASSES)\n",
        "  model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer = adam,\n",
        "              metrics=['accuracy'])\n",
        "  print(\"Iteration: \" + str(i+1) + \" of \" + str(iterations))\n",
        "  path = save_path + weight_path + weight_name + str(i) + ext\n",
        "  print(\"Weights will be saved here: \" + path)\n",
        "  model_checkpointer = ModelCheckpoint(path,\n",
        "                                       verbose = 1,\n",
        "                                       save_best_only = True,\n",
        "                                       save_weights_only = True,\n",
        "                                       mode = 'auto')\n",
        "  history = model.fit(\n",
        "      train_images,\n",
        "      train_labels,\n",
        "      verbose=1,\n",
        "      epochs = epochs,\n",
        "      batch_size = batch,\n",
        "      callbacks = [model_monitor, model_checkpointer],\n",
        "      validation_data = (val_images, val_labels)\n",
        "  )\n",
        "\n",
        "  histories.append(history)\n",
        "  loss_test, accuracy_test = model.evaluate(test_images, test_labels)\n",
        "  print('Accuracy on test data: {:4.2f}%'.format(accuracy_test * 100))\n",
        "  accuracy.append(accuracy_test)"
      ],
      "metadata": {
        "id": "xWq5BCs_LpiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Verify the best model and load it"
      ],
      "metadata": {
        "id": "dTbWzs77NP7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the index with the best accuracy\n",
        "best = [index for index, item in enumerate(accuracy) if item ==max(accuracy)]\n",
        "the_value = best[0]\n",
        "print(\"The best index is: \" + str(the_value))"
      ],
      "metadata": {
        "id": "17RlwkrbNVcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = build_resnet50(input, NUM_CLASSES)\n",
        "best_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                optimizer=adam,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "best_path = save_path + weight_path + weight_name + str(the_value) + ext\n",
        "best_model.load_weights(best_path)\n",
        "best_model.summary()\n",
        "plot_accuracy_loss(histories[the_value])\n",
        "best_loss_test, best_accuracy_test = best_model.evaluate(test_images, test_labels)\n",
        "print('Accuracy on test data: {:4.2f}%'.format(best_accuracy_test * 100))"
      ],
      "metadata": {
        "id": "oQ75IDBSNkZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save best model\n",
        "This allows us to skip retraining the model should we need to come back to this at a later time"
      ],
      "metadata": {
        "id": "kkUXvqKkNbIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.save(save_path + model_path + atd)"
      ],
      "metadata": {
        "id": "x4t_USvQOBdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2"
      ],
      "metadata": {
        "id": "7SBsECjb6DsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load in the model from Step 1"
      ],
      "metadata": {
        "id": "3guzPiesOkJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "the_model = keras.models.load_model(save_path + model_path + atd)\n",
        "the_model.summary()\n",
        "loss, acc = the_model.evaluate(test_images, test_labels, verbose=2)\n",
        "print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))"
      ],
      "metadata": {
        "id": "QncVq1pwOwlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store loss and acc for comparison later"
      ],
      "metadata": {
        "id": "lrmTt7-lPkj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "the_accuracies = []\n",
        "the_losses = []\n",
        "the_accuracies.append(acc)\n",
        "the_losses.append(loss)"
      ],
      "metadata": {
        "id": "bgoewcLMQelH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select 100 random images from the test set"
      ],
      "metadata": {
        "id": "-tHU89LHQqef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset = random_array(100, len(test_images))\n",
        "subset_images, subset_labels = get_subset(subset, test_images, test_labels)\n",
        "true_labels = subset_labels.copy()\n",
        "\n",
        "print('Subset Image shape: {} - Subset Labels shape: {}'.format(subset_images.shape, subset_labels.shape))"
      ],
      "metadata": {
        "id": "GzUfkXNvQ2w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test model on subset of images"
      ],
      "metadata": {
        "id": "-DSsYiskRplt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = the_model.evaluate(subset_images, subset_labels)\n",
        "the_accuracies.append(acc)\n",
        "the_losses.append(loss)\n",
        "print('Model performance on subset, accuracy: {:5.2f}%'.format(100 * acc))"
      ],
      "metadata": {
        "id": "OreuyOvjRxcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Classifier for Adversarial models"
      ],
      "metadata": {
        "id": "eu_YeSRxR4bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = KerasClassifier(model = the_model, clip_values=(0,1), use_logits = False)"
      ],
      "metadata": {
        "id": "zLPZbJSBSAeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adversarial Attacks"
      ],
      "metadata": {
        "id": "Lis_SYkSSUTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PGD"
      ],
      "metadata": {
        "id": "MU86XMknSHve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the images\n",
        "pgd = generate_pgd_img(classifier, subset_images)\n",
        "\n",
        "# Generate the predictions\n",
        "pgd_labels = generate_predictions(the_model, pgd)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, acc = the_model.evaluate(pgd, subset_labels)\n",
        "\n",
        "# Store the accuracy and loss\n",
        "the_accuracies.append(acc)\n",
        "the_losses.append(loss)\n",
        "\n",
        "# Print the accuracy\n",
        "print('Accuracy on pgd data: {:4.2f}%'.format(acc * 100))"
      ],
      "metadata": {
        "id": "3hCqmGc3ShI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FGSM"
      ],
      "metadata": {
        "id": "hvPw3rxgSHgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the images\n",
        "fgsm = generate_fgsm_img(classifier, subset_images)\n",
        "\n",
        "# Generate the predictions\n",
        "fgsm_labels = generate_predictions(the_model, fgsm)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, acc = the_model.evaluate(fgsm, subset_labels)\n",
        "\n",
        "# Store the accuracy and loss\n",
        "the_accuracies.append(acc)\n",
        "the_losses.append(loss)\n",
        "\n",
        "# Print the accuracy\n",
        "print('Accuracy on fgsm data: {:4.2f}%'.format(acc * 100))"
      ],
      "metadata": {
        "id": "5rZ2FMnpShoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deep Fool"
      ],
      "metadata": {
        "id": "LW94fqGHSHU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the images\n",
        "df = generate_df_img(classifier, subset_images)\n",
        "\n",
        "# Generate the predictions\n",
        "df_labels = generate_predictions(the_model, df)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, acc = the_model.evaluate(df, subset_labels)\n",
        "\n",
        "# Store the accuracy and loss\n",
        "the_accuracies.append(acc)\n",
        "the_losses.append(loss)\n",
        "\n",
        "# Print the accuracy\n",
        "print('Accuracy on deep fool data: {:4.2f}%'.format(acc * 100))"
      ],
      "metadata": {
        "id": "cJbyzfMUSiT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print Images"
      ],
      "metadata": {
        "id": "Pak1NMrqbaOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PGD"
      ],
      "metadata": {
        "id": "tYshw6oNbh_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(9, 9))\n",
        "plt.rcParams.update({'font.size': 8})\n",
        "for n in range(9):\n",
        "    i = np.random.randint(0, len(pgd), 1)\n",
        "    ax = plt.subplot(3, 3, n+1)\n",
        "    plt.imshow(pgd[i[0]].reshape(224, 224, 3))\n",
        "    plt.title(\"True Label:{}\\nPredicted Label:{}\".format(label_names[int(true_labels[i[0]])], label_names[int(pgd_labels[i[0]])]))\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "hzDStfCgbq7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FGSM"
      ],
      "metadata": {
        "id": "f3wc7zT7bh0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(9, 9))\n",
        "plt.rcParams.update({'font.size': 8})\n",
        "for n in range(9):\n",
        "    i = np.random.randint(0, len(fgsm), 1)\n",
        "    ax = plt.subplot(3, 3, n+1)\n",
        "    plt.imshow(fgsm[i[0]].reshape(224, 224, 3))\n",
        "    plt.title(\"True Label:{}\\nPredicted Label:{}\".format(label_names[int(true_labels[i[0]])], label_names[int(fgsm_labels[i[0]])]))\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "T2LaU4Rnbrmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deep Fool"
      ],
      "metadata": {
        "id": "yDy5-fRFbhp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(9, 9))\n",
        "plt.rcParams.update({'font.size': 8})\n",
        "for n in range(9):\n",
        "    i = np.random.randint(0, len(df), 1)\n",
        "    ax = plt.subplot(3, 3, n+1)\n",
        "    plt.imshow(df[i[0]].reshape(224, 224, 3))\n",
        "    plt.title(\"True Label:{}\\nPredicted Label:{}\".format(label_names[int(true_labels[i[0]])], label_names[int(df_labels[i[0]])]))\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "F-3gCMefbshP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3"
      ],
      "metadata": {
        "id": "T-O6BF3f6Dce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Defense against adversarial attacks"
      ],
      "metadata": {
        "id": "Se5KcyC6OqQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from art.attacks.evasion import BasicIterativeMethod\n",
        "from art.defences.trainer import AdversarialTrainer\n",
        "\n",
        "# Create a new classifier for the defense testing\n",
        "ATD_Classifier = KerasClassifier(clip_values=(0,1), model = the_model, use_logits = False)\n",
        "\n",
        "# Generate the attacks using the new classifier that will be used for training\n",
        "attacks = ProjectedGradientDescent(ATD_Classifier, eps=10.0/255.0, eps_step = 0.01, max_iter = 40)\n",
        "\n"
      ],
      "metadata": {
        "id": "1sSIXA3fh0JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Adversarial Training\n",
        "# As needed modify the epochs and batch size to fine tune the training for robustness\n",
        "trainer = AdversarialTrainer(ATD_Classifier, attacks, ratio=0.5)\n",
        "trainer.fit(train_images, train_labels, nb_epochs = 10, batch_size = 16)"
      ],
      "metadata": {
        "id": "uN3AAqNL1A1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ATD_Classifier = KerasClassifier(clip_values=(0,1), model = the_model, use_logits = False)\n",
        "# atd_FSGM_attack = FastGradientMethod(estimator = ATD_Classifier, eps = 20.0/255.0)\n",
        "# atd_test = atd_FSGM_attack.generate(test_images)"
      ],
      "metadata": {
        "id": "Ne6_AxMCcHcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "atd_pred = np.argmax(ATD_Classifier.predict(test_images), axis=1)\n",
        "\n",
        "# Count Correctly predicted\n",
        "correctly_predicted = np.sum(atd_pred == np.argmax(test_labels, axis = 0))\n",
        "\n",
        "# Store accuracy for all images in test data set\n",
        "the_accuracies.append(correctly_predicted/len(test_images))\n",
        "\n",
        "print('Accuracy for clean test images with Adversarially trained model: {:4.2f}%'.format((correctly_predicted/len(test_images)) * 100))"
      ],
      "metadata": {
        "id": "xHs5EOeJcHRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4"
      ],
      "metadata": {
        "id": "YXf4l_6w6DNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "atd_subset =  np.argmax(ATD_Classifier.predict(subset_images), axis=1)\n",
        "sub_predicted = np.sum(atd_subset == np.argmax(subset_labels, axis = 0))\n",
        "the_accuracies.append(sub_predicted/len(subset_images))"
      ],
      "metadata": {
        "id": "6U2EAbWNcOdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atd_pgd =  np.argmax(ATD_Classifier.predict(pgd), axis=1)\n",
        "pgd_predicted = np.sum(atd_pgd == np.argmax(subset_labels, axis = 0))\n",
        "the_accuracies.append(pgd_predicted/len(subset_images))"
      ],
      "metadata": {
        "id": "oPJTuHWTcOQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atd_fgsm =  np.argmax(ATD_Classifier.predict(fgsm), axis=1)\n",
        "fgsm_predicted = np.sum(atd_fgsm == np.argmax(subset_labels, axis = 0))\n",
        "the_accuracies.append(fgsm_predicted/len(subset_images))"
      ],
      "metadata": {
        "id": "qH8Fm3DNcOIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atd_df =  np.argmax(ATD_Classifier.predict(df), axis=1)\n",
        "df_predicted = np.sum(atd_df == np.argmax(subset_labels, axis = 0))\n",
        "the_accuracies.append(df_predicted/len(subset_images))"
      ],
      "metadata": {
        "id": "M6aAqexJcN-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display Results"
      ],
      "metadata": {
        "id": "U1RyE_GFcg25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"1 = Standard accuracy on test dataset (about 800 images)\")\n",
        "print(\"2 = Standard accuracy on the subset of 100 test images\")\n",
        "print(\"3 = Robust accuracy on FGSM attacked subset of 100 test images\")\n",
        "print(\"4 = Robust accuracy on PGD attacked subset of 100 test images\")\n",
        "print(\"5 =Robust accuracy on DF attacked subset of 100 test images\\n\")\n",
        "\n",
        "print(\"\\t\\t\\t   1\\t   2\\t   3\\t   4\\t   5\")\n",
        "print(\"Standard\\t\\t{:5.2f}%\\t{:5.2f}%\\t{:5.2f}%\\t{:5.2f}%\\t{:5.2f}%\".format(the_accuracies[0]*100, \n",
        "                                                                            the_accuracies[1]*100, \n",
        "                                                                            the_accuracies[2]*100, \n",
        "                                                                            the_accuracies[3]*100, \n",
        "                                                                            the_accuracies[4]*100\n",
        "                                                                            )\n",
        ")\n",
        "print(\"Adversarially Trained\\nClassifier\\t\\t{:5.3f}%\\t{:5.3f}%\\t{:5.3f}%\\t{:5.3f}%\\t{:5.3f}%\".format(the_accuracies[5]*100, \n",
        "                                                                                                     the_accuracies[6]*100, \n",
        "                                                                                                     the_accuracies[7]*100, \n",
        "                                                                                                     the_accuracies[8]*100, \n",
        "                                                                                                     the_accuracies[9]*100\n",
        "                                                                                                     )\n",
        ")\n"
      ],
      "metadata": {
        "id": "oOv8KUkHckAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5"
      ],
      "metadata": {
        "id": "-VNG1VX36DAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ],
      "metadata": {
        "id": "sORSEAsk6PJL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6AqWUgrJ6Bjf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}