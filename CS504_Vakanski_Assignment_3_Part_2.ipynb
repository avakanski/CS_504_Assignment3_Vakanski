{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4wqi6HaVQZYV"
      ],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM2M6hVHu0vYPjvQ0N6oqye",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jstarace/CS_504_Assignment3_Vakanski/blob/main/CS504_Vakanski_Assignment_3_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "NumlLu8tQMyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The following is a complete walk-through of Assignment #3 for CS 504: ST Adversarial Machine Learning, taught by Dr. Vakanski at the University of Idaho in the Spring of 2023.  The assignment is being made available to assist in the understanding of future related assignments at the request of Dr. Vakanski."
      ],
      "metadata": {
        "id": "zKBm5jeDQMe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## The assignment is broken down into 2 parts.\n",
        "1. Adversarial Traning defense against evasion attacks\n",
        "1. Poisoning attacks against deep learning classification models\n",
        "\n",
        "For this assignment I used Google Colab for the training of the models and completion of the assignment. It is recommend that the same or similar is used.\n"
      ],
      "metadata": {
        "id": "_HaUewYkQZn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook Setup"
      ],
      "metadata": {
        "id": "4wqi6HaVQZYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Configuration\n",
        "\n",
        "Before diving into the assignment we want to make sure our environment has all the necessary libraries and connections"
      ],
      "metadata": {
        "id": "CXcdSmQSRJzt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUA30G-BQA3r",
        "outputId": "7cfdb8fb-3bf4-487b-dbdb-ad9e06a7d48e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect to Google Drive.\n",
        "# All images required for the assignment were either provided by Dr. Vakanski or were publicly available\n",
        "# Images were loaded onto google drive for ease of access\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the adversarial robustness toolbox so that we can work with adversarial models\n",
        "!pip install adversarial-robustness-toolbox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ5HxOhDRO9U",
        "outputId": "87cadaee-96f7-4b45-abab-158946dab37f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting adversarial-robustness-toolbox\n",
            "  Downloading adversarial_robustness_toolbox-1.14.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.10.1)\n",
            "Collecting scikit-learn<1.2.0,>=0.22.2 (from adversarial-robustness-toolbox)\n",
            "  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (67.7.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (4.65.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.2.0,>=0.22.2->adversarial-robustness-toolbox) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.2.0,>=0.22.2->adversarial-robustness-toolbox) (3.1.0)\n",
            "Installing collected packages: scikit-learn, adversarial-robustness-toolbox\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed adversarial-robustness-toolbox-1.14.1 scikit-learn-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "p5X8fdE2RbKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For inline matplot use\n",
        "%matplotlib inline\n",
        "\n",
        "# Import the following libraries\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import sys\n",
        "import keras\n",
        "import random\n",
        "import imageio\n",
        "import tarfile\n",
        "import natsort\n",
        "import warnings\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras.backend as k\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "DX7OnmxMRdG8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From installed and available libraries import these specific classes\n",
        "\n",
        "from os import listdir\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.applications import vgg16\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping \n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.tools.docs.doc_controls import T\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.python.ops.math_ops import TruncateDiv\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, GlobalAveragePooling2D\n",
        "\n",
        "from art.estimators.classification import KerasClassifier\n",
        "from art.estimators.classification import SklearnClassifier\n",
        "\n",
        "from art.attacks.evasion import BoundaryAttack\n",
        "from art.attacks.evasion import FastGradientMethod,ProjectedGradientDescent,DeepFool\n",
        "from art.utils import to_categorical\n",
        "\n",
        "from skimage.util import compare_images\n",
        "from IPython.display import clear_output\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from skimage import data, transform, exposure\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "from re import U"
      ],
      "metadata": {
        "id": "6HRlPqp8Rgl-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This line is important, without it the attacks in the ART toolbox won't work\n",
        "# And it needs to be run before training the models\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# Supress warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "xvSfASGYRmQB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the version of tensorflow and keras\n",
        "print(\"TensorFlow version:{}\".format(tf.__version__))\n",
        "print(\"Keras version:{}\".format(keras.__version__))\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn-Qw1rNRoSG",
        "outputId": "5419e9e8-219c-4732-98ea-24cfc074b2ed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version:2.12.0\n",
            "Keras version:2.12.0\n",
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definitions"
      ],
      "metadata": {
        "id": "TRsvSBEfRqYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following are the definitions for methods we will use during the assignment"
      ],
      "metadata": {
        "id": "ZJsN1xbORwi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting loss and accuracy"
      ],
      "metadata": {
        "id": "WYqF-j-6RzcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple method used to print a models accuracy and loss.  Pass the method\n",
        "# the history and both accuracy and loss will be displayed\n",
        "\n",
        "def plot_accuracy_loss(the_history):\n",
        "    # plot the accuracy and loss\n",
        "    train_loss = the_history.history['loss']\n",
        "    val_loss = the_history.history['val_loss']\n",
        "    acc = the_history.history['accuracy'] \n",
        "    val_acc = the_history.history['val_accuracy']\n",
        "\n",
        "    epochsn = np.arange(1, len(train_loss)+1,1)\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(epochsn, acc, 'b', label='Training Accuracy')\n",
        "    plt.plot(epochsn, val_acc, 'r', label='Validation Accuracy')\n",
        "    plt.grid(color='gray', linestyle='--')\n",
        "    plt.legend()            \n",
        "    plt.title('ACCURACY')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(epochsn,train_loss, 'b', label='Training Loss')\n",
        "    plt.plot(epochsn,val_loss, 'r', label='Validation Loss')\n",
        "    plt.grid(color='gray', linestyle='--')\n",
        "    plt.legend()\n",
        "    plt.title('LOSS')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8_kUKGbCRsAM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Definitions"
      ],
      "metadata": {
        "id": "kcGMO4A4R7TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this assignment transfer learning was used for the base models.\n",
        "# Specifically resnet models were used.  You may note that when using the\n",
        "# models provided by Keras, their documentation mentions preprocessing\n",
        "# the images.  This is intentionally left out as it caused issues with the\n",
        "# accuracy of the predicitions.\n",
        "\n",
        "# As the inputs and classes may change we pass those as variables to provide a\n",
        "# more robust method for use.\n",
        "def build_vgg16(input_shape, class_count):\n",
        "\n",
        "  base_model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "  # Add a global spatial average pooling layer\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  # Add a fully-connected layer\n",
        "  x = Dense(2048, activation='relu')(x)\n",
        "  x = Dropout(0.25)(x)\n",
        "  x = Dense(256, activation='relu')(x)\n",
        "  x = Dropout(0.25)(x)\n",
        "  # Add a softmax layer \n",
        "  predictions = Dense(class_count, activation='softmax')(x)\n",
        "\n",
        "  # The model \n",
        "  model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "ared4m3qR9M9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper methods"
      ],
      "metadata": {
        "id": "17W4hdDwSaGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate an array of unique random ints used for selecting random indices\n",
        "def random_array(c, upper):\n",
        "  random_list = []\n",
        "  while len(random_list) < c:\n",
        "    #for i in range(c):\n",
        "    r = random.randint(0, upper-1)\n",
        "    if r not in random_list:\n",
        "      random_list.append(r)\n",
        "  return random_list\n",
        "\n",
        "# Get a subset from provided array\n",
        "def get_subset(subset, provided_images, provided_labels):\n",
        "  temp_img = []\n",
        "  temp_labels = []\n",
        "  for x in subset:\n",
        "    temp_img.append(provided_images[x])\n",
        "    temp_labels.append(provided_labels[x])\n",
        "  temp_img = np.array(temp_img, dtype = np.float32)\n",
        "  temp_labels = np.array(temp_labels, dtype = np.float32)\n",
        "  return temp_img, temp_labels"
      ],
      "metadata": {
        "id": "bFJspgBsSb5U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Loader"
      ],
      "metadata": {
        "id": "Hz_H-vCmSseE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(directory):\n",
        "\n",
        "    imgs_list = []\n",
        "\n",
        "    # List of all images in the directory\n",
        "    imgs_list_1 = listdir(directory)\n",
        "    # Make sure that the images are sorted \n",
        "    imagesList = natsort.natsorted(imgs_list_1)\n",
        "\n",
        "    # Read the images as numpy arrays\n",
        "    for i in range(len(imagesList)):\n",
        "          tmp_img = cv2.imread(os.path.join(directory, imagesList[i]))\n",
        "          img_arr = np.array(tmp_img)\n",
        "          imgs_list.append(img_arr/255.)\n",
        "    \n",
        "    # Convert the lists to numpy arrays\n",
        "    imgs = np.asarray(imgs_list)\n",
        "\n",
        "    return imgs"
      ],
      "metadata": {
        "id": "MldBs4FYSvFE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global Variables"
      ],
      "metadata": {
        "id": "eFrAu9uhS0EB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoder for loading in data\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Define a save path for use when training models\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/CS_504/Assignment_3/Saves/'\n",
        "\n",
        "# Define the path for weights to be stored\n",
        "weight_path = 'Weights/'\n",
        "\n",
        "# Define the path for the model to be stored\n",
        "model_path = 'Models/'\n",
        "\n",
        "# Define the extention for weights/models\n",
        "ext = '.h5'"
      ],
      "metadata": {
        "id": "r4CWQSL-S2Sm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ],
      "metadata": {
        "id": "96dmnFjtS73T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1"
      ],
      "metadata": {
        "id": "t5sr-wPsTUPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**About the data**\n",
        "\n",
        "Add this"
      ],
      "metadata": {
        "id": "kxbxUyuGTYmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "l61LcVeKTeqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncompress the dataset \n",
        "!unzip -uq \"/content/drive/MyDrive/Colab Notebooks/CS_504/Assignment_3/data/BUS_images.zip\" -d \"sample_data/\""
      ],
      "metadata": {
        "id": "cCAdOP-0S9j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the images\n",
        "all_images = load_images('sample_data/BUS_images')\n",
        "print('Shape of the images:', all_images.shape)\n",
        "\n",
        "# Load the labels\n",
        "all_labels = np.loadtxt('/content/drive/MyDrive/Colab Notebooks/CS_504/Assignment_3/data/labels_BUSimages.csv', delimiter=',', dtype=str)\n",
        "image_labels = le.fit_transform(all_labels)\n",
        "print('Shape of the labels:', image_labels.shape)"
      ],
      "metadata": {
        "id": "6AxMUO6HTkbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into 2 sets of data\n",
        "# 1. A trainval set that will later be split into the training and validation sets\n",
        "# 2. A test set which will be 20% of all data\n",
        "\n",
        "# By setting the random state, we can ensure consistency when creating the datasets\n",
        "# This allows us to focus on fine tuning our models\n",
        "trainval_images, test_images, trainval_labels, test_labels = train_test_split(all_images, image_labels, test_size=0.2, random_state=12)\n",
        "\n",
        "# Using the trainval sets, we split the data again into our training and validation sets\n",
        "# The validation set will be 20% of the remaining data and the remainder will be \n",
        "# our training set\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(trainval_images, trainval_labels, test_size=0.2, random_state=12)\n",
        "\n",
        "# With all the necessary info loaded, we can free up space by deleting the imported images\n",
        "del all_images"
      ],
      "metadata": {
        "id": "-K3OiwgPTvLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the shapes of train, validation, and test datasets\n",
        "print('Images train shape: {} - Labels train shape: {}'.format(train_images.shape, train_labels.shape))\n",
        "print('Images validation shape: {} - Labels validation shape: {}'.format(val_images.shape, val_labels.shape))\n",
        "print('Images test shape: {} - Labels test shape: {}'.format(test_images.shape, test_labels.shape))\n",
        "\n",
        "# Display the range of images \n",
        "print('\\nMax pixel value', np.max(train_images))\n",
        "print('Min pixel value', np.min(train_images))\n",
        "print('Average pixel value', np.mean(train_images))\n",
        "print('Data type', train_images[0].dtype)\n",
        "\n",
        "# A list with the names of the image classes\n",
        "label_names = np.unique(all_labels)\n",
        "\n",
        "del all_labels"
      ],
      "metadata": {
        "id": "YG3m3tWwT6_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display images"
      ],
      "metadata": {
        "id": "aDOsnrpjUJ3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a few images to check if the labels make sense\n",
        "plt.figure(figsize=(9, 9))\n",
        "plt.rcParams.update({'font.size': 8})\n",
        "for n in range(9):\n",
        "    i = np.random.randint(0, len(train_images), 1)\n",
        "    ax = plt.subplot(3, 3, n+1)\n",
        "    plt.imshow(train_images[i[0]])\n",
        "    plt.title('Label: ' + str(label_names[train_labels[i[0]]]))\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "8fXiNqcyULns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a deep learning model above 80% accuracy"
      ],
      "metadata": {
        "id": "1I7vde8sUeAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paths and variables"
      ],
      "metadata": {
        "id": "-gUf7edsUiFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight_name = 'bd_weights_'\n",
        "bd = 'bd/'"
      ],
      "metadata": {
        "id": "U6OU5pQYUga0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = (224, 224, 3)\n",
        "NUM_CLASSES = len(label_names)\n",
        "batch = 32\n",
        "epochs = 100\n",
        "iterations = 1"
      ],
      "metadata": {
        "id": "smQibYWTUpBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adam = tf.keras.optimizers.legacy.Adam(learning_rate = 1e-5)\n",
        "\n",
        "model_monitor = EarlyStopping(monitor='val_loss',\n",
        "                            min_delta=1e-3,\n",
        "                            patience=50,\n",
        "                            verbose=0,\n",
        "                            mode='auto')"
      ],
      "metadata": {
        "id": "mSCaBnAxUs1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the model"
      ],
      "metadata": {
        "id": "XKagehovUy-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = []\n",
        "histories = []\n",
        "\n",
        "for i in range(iterations):\n",
        "  model = build_vgg19(input, NUM_CLASSES)\n",
        "  model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer = adam,\n",
        "              metrics=['accuracy'])\n",
        "  print(\"Iteration: \" + str(i+1) + \" of \" + str(iterations))\n",
        "  path = save_path + weight_path + weight_name + str(i) + ext\n",
        "  print(\"Weights will be saved here: \" + path)\n",
        "  model_checkpointer = ModelCheckpoint(path,\n",
        "                                       verbose = 1,\n",
        "                                       save_best_only = True,\n",
        "                                       save_weights_only = True,\n",
        "                                       mode = 'auto')\n",
        "  history = model.fit(\n",
        "      train_images,\n",
        "      train_labels,\n",
        "      verbose=1,\n",
        "      epochs = epochs,\n",
        "      batch_size = batch,\n",
        "      callbacks = [model_monitor, model_checkpointer],\n",
        "      validation_data = (val_images, val_labels)\n",
        "  )\n",
        "\n",
        "  histories.append(history)\n",
        "  loss_test, accuracy_test = model.evaluate(test_images, test_labels)\n",
        "  print('Accuracy on test data: {:4.2f}%'.format(accuracy_test * 100))\n",
        "  accuracy.append(accuracy_test)"
      ],
      "metadata": {
        "id": "Sgvnp0xfU2ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Validate the best model and load it"
      ],
      "metadata": {
        "id": "UoBtj8_CVJN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Print the index with the best accuracy\n",
        "best = [index for index, item in enumerate(accuracy) if item ==max(accuracy)]\n",
        "the_value = best[0]\n",
        "print(\"The best index is: \" + str(the_value))"
      ],
      "metadata": {
        "id": "269IcPTyVNCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = build_vgg16(input, NUM_CLASSES)\n",
        "best_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                optimizer=adam,\n",
        "                metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Jy1dljmBVQFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_path = save_path + weight_path + weight_name + str(the_value) + ext\n",
        "best_model.load_weights(best_path)\n",
        "best_model.summary()\n",
        "plot_accuracy_loss(histories[the_value])\n",
        "best_loss_test, best_accuracy_test = best_model.evaluate(test_images, test_labels)\n",
        "print('Accuracy on test data: {:4.2f}%'.format(best_accuracy_test * 100))"
      ],
      "metadata": {
        "id": "h17DjXJVV0lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save best model\n",
        "This allows us to skip retraining the model should we need to come back to this at a later time"
      ],
      "metadata": {
        "id": "CGIqCnZuV5A-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.save(save_path + model_path + bd)"
      ],
      "metadata": {
        "id": "fScznH_-V0ZJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}